{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "\n",
    "# Number of train/test files\n",
    "num_train_file = 10000\n",
    "num_test_file = 2000\n",
    "\n",
    "# Functions for removing noise and garbage of the data\n",
    "def strip_punctuation(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]=''.join([letter for letter in data[i] if letter not in punctuation])\n",
    "    return data\n",
    "\n",
    "def lowercase(data):\n",
    "    return [x.lower() for x in data]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def strip_stop_words(data):\n",
    "    for i in range(len(data)):\n",
    "        word_tokens = word_tokenize(data[i])\n",
    "        filtered_sentence = []\n",
    "        for word in word_tokens:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        data[i] = \" \".join(filtered_sentence)\n",
    "    return data\n",
    "\n",
    "def lemmatizer(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(data)):\n",
    "        word_list = data[i].split(' ')\n",
    "        temp = []\n",
    "        for word in word_list:\n",
    "            temp.append(lemmatizer.lemmatize(word, 'v'))\n",
    "        data[i] = \" \".join(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_desc_dict(path, num_file):\n",
    "    \n",
    "    desc_dict = {}\n",
    "    for i in range(num_file):\n",
    "        filename = path + str(i) +'.txt'\n",
    "        file_number = (filename.split('/')[-1]).split('.')[0]\n",
    "        \n",
    "        # Read files.\n",
    "        file = open(filename, \"r\")\n",
    "        contents = [line.rstrip(\"\\n\") for line in file]\n",
    "        \n",
    "        # Lowercase all of the words.\n",
    "        contents = lowercase(contents)\n",
    "        \n",
    "        # Strip punctuation\n",
    "        contents = strip_punctuation(contents)\n",
    "        \n",
    "        # Strip the stop words\n",
    "        contents = strip_stop_words(contents)\n",
    "        \n",
    "        # Lemmatization of all the words\n",
    "        contents = lemmatizer(contents)\n",
    "        \n",
    "        desc_dict[file_number] = contents\n",
    "    \n",
    "    return desc_dict\n",
    "\n",
    "# Generate train/test description dictionaries\n",
    "train_desc_dict = generate_desc_dict('./data/descriptions_train/', num_train_file)\n",
    "test_desc_dict = generate_desc_dict('./data/descriptions_test/', num_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_indexed_desc_dict(desc_dict):\n",
    "    \n",
    "    indexed_desc_dict = {}\n",
    "    desc_list = []\n",
    "    for file_num, descriptions in desc_dict.items():\n",
    "        for desc in descriptions:\n",
    "            words = desc.split()\n",
    "            for word in words:\n",
    "                desc_list.append(word)\n",
    "                \n",
    "    desc_list = list(set(desc_list))\n",
    "    \n",
    "    index = 0\n",
    "    for desc in desc_list:\n",
    "        indexed_desc_dict[desc] = index\n",
    "        index += 1\n",
    "        \n",
    "    return indexed_desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7658\n"
     ]
    }
   ],
   "source": [
    "indexed_desc_dict = generate_unique_indexed_desc_dict(train_desc_dict)\n",
    "print(len(indexed_desc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\n",
    "\n",
    "# Generate two-dimensional data structure (table)\n",
    "def generate_desc_vectors(desc_dict, num_file):\n",
    "    desc_vector = np.zeros((num_file, len(indexed_desc_dict.keys())))\n",
    "    \n",
    "    for file_num, descs in desc_dict.items():\n",
    "        for desc in descs:\n",
    "            words = desc.split()\n",
    "            for word in words:                \n",
    "                if word in indexed_desc_dict.keys():\n",
    "                    desc_vector[int(file_num), int(indexed_desc_dict[word])] += 1\n",
    "                \n",
    "    return pd.DataFrame(desc_vector, columns = indexed_desc_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7658)\n"
     ]
    }
   ],
   "source": [
    "# Create train/test binary description vectors (table)\n",
    "train_desc_vectors = generate_desc_vectors(train_desc_dict, num_train_file)\n",
    "test_desc_vectors = generate_desc_vectors(test_desc_dict, num_test_file)\n",
    "print(desc_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tag_dict(file_path, num_file):\n",
    "    \n",
    "    tags_dict = {}\n",
    "    sub_category = []\n",
    "    for i in range(num_file):\n",
    "        filename = file_path + str(i) +'.txt'\n",
    "        file_number = (filename.split('/')[-1]).split('.')[0]\n",
    "        \n",
    "        # Read files.\n",
    "        file = open(filename, \"r\")\n",
    "        tags = [line.rstrip(\"\\n\") for line in file]\n",
    "        \n",
    "        temp = []\n",
    "        for tag in tags:\n",
    "            tag = tag.split(':')\n",
    "            temp.append(tag[1])\n",
    "        tags_dict[file_number] = temp\n",
    "            \n",
    "    return tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate train/test tag dictionary.\n",
    "train_tags_dict = generate_tag_dict('./data/tags_train/', num_train_file)\n",
    "test_tags_dict = generate_tag_dict('./data/tags_test/', num_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_indexed_tag_dict(tag_dict):\n",
    "    \n",
    "    indexed_tag_dict = {}\n",
    "    tag_list = []\n",
    "    for tags in tag_dict.values():\n",
    "        for tag in tags:\n",
    "            tag_list.append(tag)\n",
    "                \n",
    "    tag_list = list(set(tag_list))\n",
    "    \n",
    "    index = 0\n",
    "    for tag in tag_list:\n",
    "        indexed_tag_dict[tag] = index\n",
    "        index += 1\n",
    "        \n",
    "    return indexed_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "indexed_tag_dict = generate_unique_indexed_tag_dict(train_tags_dict)\n",
    "print(len(indexed_tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html\n",
    "\n",
    "# Generate two-dimensional data structure (table)\n",
    "def generate_tags_vector(tags_dict, num_file):\n",
    "    tag_vector = np.zeros((num_file, len(indexed_tag_dict.keys())))\n",
    "    \n",
    "    for file_num, tags in tags_dict.items():\n",
    "        for tag in tags:\n",
    "            if tag in indexed_tag_dict.keys():\n",
    "                tag_vector[int(file_num), int(indexed_tag_dict[tag])] += 1\n",
    "                \n",
    "    return pd.DataFrame(tag_vector, columns = indexed_tag_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "# Create train/test binary tag vectors (tables)\n",
    "train_tag_vectors = generate_tags_vector(train_tags_dict, num_train_file)\n",
    "test_tag_vectors = generate_tags_vector(test_tags_dict, num_test_file)\n",
    "print(test_tag_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/18594469/normalizing-a-pandas-dataframe-by-row\n",
    "# Normalize train/test vectors to be less sensitive to the scale of features\n",
    "from sklearn import preprocessing\n",
    "train_normalized_desc_vector = train_desc_vectors.div(train_desc_vectors.sum(axis=1), axis=0)\n",
    "test_normalized_desc_vector = test_desc_vectors.div(test_desc_vectors.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_normalized_desc_vector)\n",
    "# print(tag_vectors)\n",
    "# print(test_normalized_desc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/modules/multiclass.html\n",
    "# https://www.programcreek.com/python/example/94869/sklearn.multiclass.OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "def SVM(train, trainLabel, test):\n",
    "    classifier = OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "    classifier.fit(train, trainLabel)\n",
    "    predictions = classifier.predict(test)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Perform SVM\n",
    "test_predic = SVM(train_normalized_desc_vector.values, tag_vectors.values, test_normalized_desc_vector.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 80)\n"
     ]
    }
   ],
   "source": [
    "print(test_predic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# https://stackoverflow.com/questions/37782049/sklearn-kneighbours-memory-error-python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def kNN(tag_vectors, k, predic):\n",
    "    neigh = NearestNeighbors(n_neighbors= k , algorithm='kd_tree').fit(tag_vectors)\n",
    "    dist, idx = neigh.kneighbors(predic)\n",
    "    return dist, idx\n",
    "\n",
    "# Perform knn\n",
    "dist, predic_idx = kNN(test_tag_vectors, 20, test_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 292   36 1743 ...  897  281  760]\n",
      " [ 833  589 1529 ...  784  948  758]\n",
      " [1724  649 1866 ... 1033 1059  953]\n",
      " ...\n",
      " [ 104 1481 1726 ...  429   58  235]\n",
      " [1218 1660 1535 ...   63   70   28]\n",
      " [1342  653 1430 ...  346  405 1129]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pythonspot.com/save-a-dictionary-to-a-file/\n",
    "import csv\n",
    "\n",
    "# Create top 20 dictionary\n",
    "result_dic = {}\n",
    "for i in range(2000):\n",
    "    desc_id = str(i) + '.txt'\n",
    "    temp = []\n",
    "    images = preds[i]\n",
    "    for image in images:\n",
    "        temp.append(str(image) + '.jpg')\n",
    "    top_20 = \" \".join(temp)\n",
    "    result_dic[desc_id] = top_20\n",
    "\n",
    "# Write csv file using the dictionary above\n",
    "w = csv.writer(open(\"submission_3.csv\", \"w\", newline=''))\n",
    "w.writerow([\"Descritpion_ID\", \"Top_20_Image_IDs\"])\n",
    "for key, val in result_dic.items():\n",
    "    w.writerow([key, val])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
