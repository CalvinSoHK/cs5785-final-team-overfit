{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for removing noise and garbage of the data\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "# Number of train/test files\n",
    "num_train_file = 10000\n",
    "num_test_file = 2000\n",
    "\n",
    "def strip_punctuation(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i]=''.join([letter for letter in data[i] if letter not in punctuation])\n",
    "    return data\n",
    "\n",
    "def lowercase(data):\n",
    "    return [x.lower() for x in data]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def strip_stop_words(data):\n",
    "    for i in range(len(data)):\n",
    "        word_tokens = word_tokenize(data[i])\n",
    "        filtered_sentence = []\n",
    "        for word in word_tokens:\n",
    "            if word not in stop_words:\n",
    "                filtered_sentence.append(word)\n",
    "        data[i] = \" \".join(filtered_sentence)\n",
    "    return data\n",
    "\n",
    "def lemmatizer(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(data)):\n",
    "        word_list = data[i].split(' ')\n",
    "        temp = []\n",
    "        for word in word_list:\n",
    "            temp.append(lemmatizer.lemmatize(word, 'v'))\n",
    "        data[i] = \" \".join(temp)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_unique_words_dict(desc_train_path):\n",
    "    \n",
    "    unique_words_dict = []\n",
    "    for i in range(num_train_file):\n",
    "        filename = desc_train_path + str(i) +'.txt'\n",
    "        \n",
    "        # Read files.\n",
    "        file = open(filename, \"r\")\n",
    "        contents = [line.rstrip(\"\\n\") for line in file]\n",
    "        \n",
    "        # Lowercase all of the words.\n",
    "        contents = lowercase(contents)\n",
    "        \n",
    "        # Strip punctuation\n",
    "        contents = strip_punctuation(contents)\n",
    "        \n",
    "        # Strip the stop words\n",
    "        contents = strip_stop_words(contents)\n",
    "        \n",
    "        # Lemmatization of all the words\n",
    "        contents = lemmatizer(contents)\n",
    "        \n",
    "        for content in contents:\n",
    "            words = content.split()\n",
    "            for word in words:\n",
    "                unique_words_dict.append(word)\n",
    "    \n",
    "    return set(unique_words_dict)\n",
    "        \n",
    "unique_words_dict = generate_unique_words_dict('./data/descriptions_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a set of unique words from tags\n",
    "def generate_unique_words_dict_tag(tag_train_path):\n",
    "    \n",
    "    unique_words_dict = []\n",
    "    for i in range(num_train_file):\n",
    "        filename = tag_train_path + str(i) +'.txt'\n",
    "        \n",
    "        # Read files.\n",
    "        file = open(filename, \"r\")\n",
    "        contents = [line.rstrip(\"\\n\") for line in file]\n",
    "        \n",
    "        # Lowercase all of the words.\n",
    "        contents = lowercase(contents)\n",
    "        \n",
    "        # Strip punctuation\n",
    "        contents = strip_punctuation(contents)\n",
    "        \n",
    "        # Strip the stop words\n",
    "        contents = strip_stop_words(contents)\n",
    "        \n",
    "        # Lemmatization of all the words\n",
    "        contents = lemmatizer(contents)\n",
    "        \n",
    "        for content in contents:\n",
    "            words = content.split()\n",
    "            for entry in words:\n",
    "                pair = entry.split(':')\n",
    "                for word in pair:\n",
    "                    unique_words_dict.append(word)\n",
    "    \n",
    "    return set(unique_words_dict)\n",
    "\n",
    "# Combines the unique words from descriptions and tags together into one dictionary. No duplicates.\n",
    "unique_words_dict = list(unique_words_dict.union(generate_unique_words_dict_tag('./data/tags_train/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the unique words dictionary only has one of each word\n",
    "for word in unique_words_dict:\n",
    "    if unique_words_dict.count(word) > 1:\n",
    "        print('Bad!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Uses unique_words_dict to ensure only words learned from training data is read\n",
    "def generate_bag_of_words(file_path, num_file):\n",
    "    bag_of_words = {}\n",
    "    count = 0\n",
    "    for i in range(num_file):\n",
    "        filename = file_path + str(i) +'.txt'\n",
    "        \n",
    "        # Read files.\n",
    "        file = open(filename, \"r\")\n",
    "        contents = [line.rstrip(\"\\n\") for line in file]        \n",
    "        \n",
    "        # Create a list with contents\n",
    "        temp = ''\n",
    "        for content in contents:\n",
    "            temp += ' ' + content\n",
    "        contents = [temp]\n",
    "    \n",
    "        # Lowercase all of the words.\n",
    "        contents = lowercase(contents)\n",
    "        \n",
    "        # Strip punctuation\n",
    "        contents = strip_punctuation(contents)\n",
    "        \n",
    "        # Strip the stop words\n",
    "        contents = strip_stop_words(contents)\n",
    "        \n",
    "        # Lemmatization of all the words\n",
    "        contents = lemmatizer(contents)\n",
    "        \n",
    "        for content in contents:\n",
    "            words = content.split()\n",
    "            counts = []\n",
    "            for word in unique_words_dict:\n",
    "                counts.append(words.count(word))\n",
    "            \n",
    "            new_file_name = filename.split('/')[-1]\n",
    "            bag_of_words[new_file_name] = counts\n",
    "            \n",
    "        if count % 1000 == 0:\n",
    "            print('Progress: ' + str(count) + '/' + str(num_file))\n",
    "        count += 1\n",
    "            \n",
    "    return bag_of_words\n",
    "        \n",
    "training_bag_of_words = generate_bag_of_words('./data/descriptions_train/', num_train_file)\n",
    "test_bag_of_words = generate_bag_of_words('./data/descriptions_test/', num_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/10000\n",
      "Progress: 1000/10000\n",
      "Progress: 2000/10000\n",
      "Progress: 3000/10000\n",
      "Progress: 4000/10000\n",
      "Progress: 5000/10000\n",
      "Progress: 6000/10000\n",
      "Progress: 7000/10000\n",
      "Progress: 8000/10000\n",
      "Progress: 9000/10000\n",
      "Progress: 0/2000\n",
      "Progress: 1000/2000\n"
     ]
    }
   ],
   "source": [
    "# Similar as above but reads both descriptions and tags\n",
    "def generate_bag_of_words_combined(desc_path, tag_path, num_file):\n",
    "    bag_of_words = {}\n",
    "    count = 0\n",
    "    for i in range(num_file):\n",
    "        desc_file = desc_path + str(i) +'.txt'\n",
    "        tag_file = tag_path + str(i) + '.txt'\n",
    "        \n",
    "        # Read files.\n",
    "        desc_file_open = open(desc_file, \"r\")\n",
    "        desc_contents = [line.rstrip(\"\\n\") for line in desc_file_open]  \n",
    "        tag_file_open = open(tag_file, \"r\")\n",
    "        tag_contents = [line.rstrip(\"\\n\") for line in tag_file_open]    \n",
    "        contents = []\n",
    "        \n",
    "        # Split pairs from tags\n",
    "        # NOTE set up so tags are worth more than description. Multiples of tags will be added to weigh it more heavily.\n",
    "        for content in tag_contents:\n",
    "            words = content.split()\n",
    "            for entry in words:\n",
    "                pair = entry.split(':')\n",
    "                if len(pair) == 2:\n",
    "                    contents.append(pair[0])\n",
    "                    contents.append(pair[0])\n",
    "                    contents.append(pair[1])\n",
    "                    contents.append(pair[1])\n",
    "                    contents.append(pair[1])\n",
    "                    \n",
    "        # Add all words from description file\n",
    "        for content in desc_contents:\n",
    "            words = content.split()\n",
    "            for word in words:\n",
    "                contents.append(word)\n",
    "        \n",
    "        # Create a list with contents\n",
    "        temp = ''\n",
    "        for content in contents:\n",
    "            temp += ' ' + content\n",
    "        contents = [temp]\n",
    "    \n",
    "        # Lowercase all of the words.\n",
    "        contents = lowercase(contents)\n",
    "        \n",
    "        # Strip punctuation\n",
    "        contents = strip_punctuation(contents)\n",
    "        \n",
    "        # Strip the stop words\n",
    "        contents = strip_stop_words(contents)\n",
    "        \n",
    "        # Lemmatization of all the words\n",
    "        contents = lemmatizer(contents)\n",
    "        \n",
    "        for content in contents:\n",
    "            words = content.split()\n",
    "            counts = []\n",
    "            for word in unique_words_dict:\n",
    "                counts.append(words.count(word))\n",
    "            \n",
    "            new_file_name = i\n",
    "            bag_of_words[new_file_name] = counts\n",
    "            \n",
    "        if count % 1000 == 0:\n",
    "            print('Progress: ' + str(count) + '/' + str(num_file))\n",
    "        count += 1\n",
    "            \n",
    "    return bag_of_words\n",
    "        \n",
    "training_bag_of_words = generate_bag_of_words_combined('./data/descriptions_train/', './data/tags_train/', num_train_file)\n",
    "test_bag_of_words = generate_bag_of_words_combined('./data/descriptions_test/', './data/tags_test/', num_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train/test features.\n",
    "train_features = []\n",
    "test_features = []\n",
    "\n",
    "train_feature_file = open(\"./data/features_train/features_resnet1000_train.csv\", \"r\")\n",
    "train_features = np.array([line.split(\",\") for line in train_feature_file])\n",
    "\n",
    "test_feature_file = open(\"./data/features_test/features_resnet1000_test.csv\", \"r\")\n",
    "test_features = np.array([line.split(\",\") for line in test_feature_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create knn model for training descriptions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "X, y = [], []\n",
    "for filename in training_bag_of_words:\n",
    "    X.append(training_bag_of_words[filename])\n",
    "    y.append(filename)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "knn_classifier.fit(X, y)\n",
    "\n",
    "# def process_knn(desc_test_file):\n",
    "\n",
    "#     knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "#     X, y = [], []\n",
    "#     for filename in training_bag_of_words:\n",
    "#         X.append(training_bag_of_words[filename])\n",
    "#         y.append(filename)\n",
    "\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "#     knn_classifier.fit(X, y)\n",
    "    \n",
    "#     bag_of_word = np.array(test_bag_of_words[desc_test_file])\n",
    "#     train_file_name = knn_classifier.predict(bag_of_word.reshape(1, -1))\n",
    "    \n",
    "#     return train_file_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train feature dictionary\n",
    "train_feature_dict = {}\n",
    "for i  in range(len(train_features)):\n",
    "    file_name = train_features[i][0] # 'images_train/5373.jpg'\n",
    "    file_name = file_name.split('/')[1].split('.')[0] # 5373\n",
    "    train_feature_dict[file_name] = np.array(train_features[i][1:])\n",
    "\n",
    "# def get_train_feature(train_file_name):\n",
    "    \n",
    "#     train_file_name = train_file_name.split('.')[0]\n",
    "    \n",
    "#     # Return train feature with train file name\n",
    "#     return train_feature_dict[train_file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "def get_euclidean_distance(a, b):\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "import operator\n",
    "\n",
    "def get_top_20_images(desc_test_file):   \n",
    "    index = int(desc_test_file.split('.')[0])\n",
    "    bag_of_word = np.array(test_bag_of_words[index])\n",
    "    train_file_name = knn_classifier.predict(bag_of_word.reshape(1, -1)) # ['1234.jpg']\n",
    "#     train_file_name = process_knn(desc_test_file)\n",
    "    selected_train_feature = train_feature_dict[str(train_file_name[0])]\n",
    "#     selected_train_feature = get_train_feature(train_file_name)\n",
    "    \n",
    "    feature_dist = {}\n",
    "    for test_feature in test_features:\n",
    "        test_file_name = test_feature[0].split(\"/\")[1]\n",
    "        dist = get_euclidean_distance(selected_train_feature, test_feature[1:])\n",
    "        feature_dist[test_file_name] = dist\n",
    "    sorted_feature_dist = sorted(feature_dist.items(), key=operator.itemgetter(1))\n",
    "    top_20 = []\n",
    "    for i in range(20):\n",
    "        top_20.append(sorted_feature_dist[i][0])\n",
    "\n",
    "    return np.array(top_20)\n",
    "\n",
    "# top_20 = get_top_20_images(\"5.txt\")\n",
    "# print(top_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://stackoverflow.com/questions/42812230/why-plt-imshow-doesnt-display-the-image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def display_top_20():\n",
    "    for image_name in top_20:\n",
    "        image = Image.open('./data/images_test/' + image_name, 'r')\n",
    "        plt.imshow(np.asarray(image))\n",
    "        plt.show()\n",
    "        \n",
    "# display_top_20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/2000\n",
      "Progress: 50/2000\n",
      "Progress: 100/2000\n",
      "Progress: 150/2000\n",
      "Progress: 200/2000\n",
      "Progress: 250/2000\n",
      "Progress: 300/2000\n",
      "Progress: 350/2000\n",
      "Progress: 400/2000\n",
      "Progress: 450/2000\n",
      "Progress: 500/2000\n",
      "Progress: 550/2000\n",
      "Progress: 600/2000\n",
      "Progress: 650/2000\n",
      "Progress: 700/2000\n",
      "Progress: 750/2000\n",
      "Progress: 800/2000\n",
      "Progress: 850/2000\n",
      "Progress: 900/2000\n",
      "Progress: 950/2000\n",
      "Progress: 1000/2000\n",
      "Progress: 1050/2000\n",
      "Progress: 1100/2000\n",
      "Progress: 1150/2000\n",
      "Progress: 1200/2000\n",
      "Progress: 1250/2000\n",
      "Progress: 1300/2000\n",
      "Progress: 1350/2000\n",
      "Progress: 1400/2000\n",
      "Progress: 1450/2000\n",
      "Progress: 1500/2000\n",
      "Progress: 1550/2000\n",
      "Progress: 1600/2000\n",
      "Progress: 1650/2000\n",
      "Progress: 1700/2000\n",
      "Progress: 1750/2000\n",
      "Progress: 1800/2000\n",
      "Progress: 1850/2000\n",
      "Progress: 1900/2000\n",
      "Progress: 1950/2000\n",
      "9100.887622594833 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# dic = {\"John\": \"john@example.com\", \"Mary\": \"mary@example.com\"} #dictionary\n",
    "count = 0\n",
    "result_dic = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(2000):\n",
    "    desc_test_name = str(i) + '.txt'\n",
    "    top_20_imgs = \" \".join(get_top_20_images(desc_test_name))\n",
    "    result_dic[desc_test_name] = top_20_imgs\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        print('Progress: ' + str(count) + '/' + str(2000))\n",
    "    count += 1\n",
    "    \n",
    "print(str(time.time() - start_time) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reference: https://pythonspot.com/save-a-dictionary-to-a-file/\n",
    "import csv\n",
    "\n",
    "w = csv.writer(open(\"submission1.csv\", \"w\", newline=''))\n",
    "w.writerow([\"Descritpion_ID\", \"Top_20_Image_IDs\"])\n",
    "for key, val in result_dic.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation check with model\n",
    "def GetAccuracy()\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "X, y = [], []\n",
    "for filename in training_bag_of_words:\n",
    "    X.append(training_bag_of_words[filename])\n",
    "    y.append(filename)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "knn_classifier.fit(X, y)\n",
    "\n",
    "count = 0\n",
    "result_dic = {}\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(2000):\n",
    "    desc_test_name = str(i) + '.txt'\n",
    "    top_20_imgs = \" \".join(get_top_20_images(desc_test_name))\n",
    "    result_dic[desc_test_name] = top_20_imgs\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        print('Progress: ' + str(count) + '/' + str(2000))\n",
    "    count += 1\n",
    "    \n",
    "print(str(time.time() - start_time) + ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OUTIS\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\OUTIS\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create knn model for training descriptions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "regression = LogisticRegression()\n",
    "X, y = [], []\n",
    "for filename in training_bag_of_words:\n",
    "    X.append(training_bag_of_words[filename])\n",
    "    y.append(filename)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features per sample; expecting 7738",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-b7c81d3ad7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_bag_of_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_bag_of_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \"\"\"\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 262\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features per sample; expecting 7738"
     ]
    }
   ],
   "source": [
    "print(np.array(test_bag_of_words[0]))\n",
    "regression.predict(np.array(test_bag_of_words[0]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
