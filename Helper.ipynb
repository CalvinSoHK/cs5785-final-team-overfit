{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import scripts we need\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.model_selection import train_test_split as cross_validation\n",
    "\n",
    "#This is the stop words set\n",
    "stop_words = set(stopwords.words('english')) \n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocesses text for us\n",
    "def PreProcess(text = str):\n",
    "    return_list = []   \n",
    "    wnl = WordNetLemmatizer()\n",
    "    text_list = text.lower().split()\n",
    "    for word in text_list:\n",
    "        word = ''.join(ch for ch in word if ch not in punctuation)\n",
    "        if word not in stop_words:\n",
    "            return_list.append(wnl.lemmatize(word, 'v'))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reads data from a single file and sends it back as one line\n",
    "# NOTE: Lowercases, strips, and lemmatizes all words\n",
    "def GetDescFileData(path = str):\n",
    "    # Open file path\n",
    "    file_path = open(path, 'r')\n",
    "    \n",
    "    # Read line by line and append them all into data\n",
    "    line = file_path.readline()\n",
    "    data = []\n",
    "    while line:  \n",
    "        processed_lines = PreProcess(line)\n",
    "        for entry in processed_lines:\n",
    "            data.append(entry)\n",
    "        line = file_path.readline()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reads all description data\n",
    "# Outputs data as a list. First index is test data, second is training.\n",
    "def GetDescriptionData():\n",
    "    data = []\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for i in range(10000):\n",
    "        train_data.append(GetDescFileData('data/descriptions_train/' + str(i)  + '.txt'))    \n",
    "    for i in range(2000):\n",
    "        test_data.append(GetDescFileData('data/descriptions_test/' + str(i) + '.txt'))    \n",
    "    data.append(np.array(train_data))\n",
    "    data.append(np.array(test_data))\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Example of desc data\n",
    "\n",
    "# desc_data = GetDescriptionData()\n",
    "# print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reads all tag data from a single file\n",
    "def GetTagFileData(path = str):\n",
    "    # Open file path\n",
    "    file_path = open(path, 'r')\n",
    "    \n",
    "   # Read line by line and append them all into data\n",
    "    line = file_path.readline()\n",
    "    data = []\n",
    "    while line:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        split_line = line.split(':')\n",
    "        line = (split_line[0].lower().strip(), split_line[1].lower().strip())\n",
    "        for word in line:\n",
    "            if word not in stop_words:\n",
    "                word = wnl.lemmatize(word, 'v')\n",
    "        data.append(line)\n",
    "        line = file_path.readline()\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reads all tag data\n",
    "def GetTagData():\n",
    "    data = []\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for i in range(2000):\n",
    "        test_data.append(GetTagFileData('data/tags_test/' + str(i) + '.txt'))\n",
    "    for i in range(10000):\n",
    "        train_data.append(GetTagFileData('data/tags_train/' + str(i) + '.txt'))\n",
    "    data.append(np.array(train_data))\n",
    "    data.append(np.array(test_data))\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new tags from description and add to tags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "#Generates tags from description data according to which indexes you want.\n",
    "def CombineDescIntoTags(desc_data = [], tag_data = []):\n",
    "    combined_list = []\n",
    "    for i in range(len(desc_data)):\n",
    "        word_list = []\n",
    "        new_list = []\n",
    "        tag_data_list = tag_data[i].tolist()\n",
    "        for pair in tag_data_list:\n",
    "            word_list.append(pair[0])\n",
    "            word_list.append(pair[1])\n",
    "        for word in desc_data[i]:\n",
    "            word_list.append(word)\n",
    "        for word in word_list:\n",
    "            if word_list.count(word) >= 2:\n",
    "                new_list.append(word)\n",
    "        combined_list.append(new_list)\n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_data = GetDescriptionData()\n",
    "# tag_data = GetTagData()\n",
    "# combined_train_data = CombineDescIntoTags(desc_data[0], tag_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combined_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert tags to the same format as desc\n",
    "def ConvertTagToDescFormat(tag_data = []):\n",
    "    new_list = []\n",
    "    for entry in tag_data:\n",
    "        new_entry = []\n",
    "        for pairs in entry:\n",
    "            new_entry.append(pairs[0])\n",
    "            new_entry.append(pairs[1])\n",
    "        new_list.append(new_entry)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of getting tag data\n",
    "# tag_data = GetTagData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reads data from one csv file \n",
    "# NOTE: Can't convert to numpy array, causes memory error.\n",
    "def GetFeaturesFileData(path = str):\n",
    "     # Open file path\n",
    "    file_path = open(path, 'r')\n",
    "    csv_reader = csv.reader(file_path, delimiter=',')\n",
    "    \n",
    "    # Read the csv row by row\n",
    "    data = []\n",
    "    for row in csv_reader:\n",
    "        data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GetFeaturesFileData('data/features_train/features_resnet1000_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display image function, takes index\n",
    "def DisplayImages(list = [], data_type = str):\n",
    "    for index in list:\n",
    "        DisplayImage(index, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display a single image\n",
    "def DisplayImage(index = int, data_type =str):\n",
    "    if data_type is 'train':\n",
    "        image = mpimg.imread('data/images_train/' + str(index) + '.jpg')\n",
    "    elif data_type is 'test':\n",
    "        image = mpimg.imread('data/images_test/' + str(index) + '.jpg')\n",
    "    print('Image ' + str(index) + '\\n')\n",
    "    imgplot = plt.imshow(image)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of using DisplayImages\n",
    "# DisplayImages([0,1,2], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Return a list of words of all words in the given data set \n",
    "# Input lists should be a list of lists of words.\n",
    "def GetBagOfWordsVector (data = []):\n",
    "    words = []\n",
    "    for entry in data:\n",
    "        for word in entry:\n",
    "            if word not in words:\n",
    "                words.append(word)\n",
    "    return np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of getting the bag of words vector from data.\n",
    "# print(desc_data[0])\n",
    "# DESC_BAG_VECTOR = GetBagOfWordsVector(desc_data[0])\n",
    "# print(DESC_BAG_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converts a given list of words and converts it into a bag of words representation based on the bag of words vector provided\n",
    "def ConvertToBagVector(data = [], bag_vector = []):\n",
    "    vector = [0 for i in range(len(bag_vector))]\n",
    "    bag_vector = bag_vector.tolist()\n",
    "    for word in data:\n",
    "        if word in bag_vector:\n",
    "            # print(word)\n",
    "            vector[bag_vector.index(word)] += 1 \n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is an example that prints out a bag vector from index 20 from the training description data\n",
    "# convert1 = ConvertToBagVector(desc_data[1][20], DESC_BAG_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Returns a numpy array of all descriptions as a bag of words\n",
    "def ConvertDescToBagVector(data = [], bag_vector = []):\n",
    "    return_data = []\n",
    "    for desc in data:\n",
    "        return_data.append(ConvertToBagVector(desc, bag_vector))\n",
    "    return np.array(return_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below are the bag vectors of just the descriptions\n",
    "# DESC_BAG_VECTORS = ConvertDescToBagVector(desc_data[0], DESC_BAG_VECTOR)\n",
    "# TEST_DESC_BAG_VECTORS = ConvertDescToBagVector(desc_data[1], DESC_BAG_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Combines description info and tag info\n",
    "def CombineDescAndTags(desc_data = [], tag_data = []):\n",
    "    return_data = []\n",
    "    for i in range(len(desc_data)):\n",
    "        new_line = []\n",
    "        for word in desc_data[i]:\n",
    "            new_line.append(word)\n",
    "        for tag in tag_data[i]:\n",
    "            for word in tag:\n",
    "                new_line.append(word)\n",
    "        return_data.append(np.array(new_line))\n",
    "    return np.array(return_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the combined data of desc and tags with no weighting to anything\n",
    "# COMBINED_FLAT_DATA = CombineDescAndTags(desc_data[0], tag_data[0])\n",
    "# COMBINED_FLAT_TEST_DATA = CombineDescAndTags(desc_data[1], tag_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the combined data and desc with tags flat bag of words vector\n",
    "# COMBINED_FLAT_BAG_VECTOR = GetBagOfWordsVector(COMBINED_FLAT_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the conversion of the flat data into flat bag vectors\n",
    "# FLAT_BAG_DATA = ConvertDescToBagVector(COMBINED_FLAT_DATA, COMBINED_FLAT_BAG_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combines description info and tag info with weights\n",
    "# Weights are done just by adding the word multiple times.\n",
    "# NOTE: Tag weights are. Sub = 2. Super = 3.\n",
    "def CombineDescAndTagsWeighted(desc_data = [], tag_data = []):\n",
    "    return_data = []\n",
    "    for i in range(len(desc_data)):\n",
    "        new_line = []\n",
    "        for word in desc_data[i]:\n",
    "            new_line.append(word)\n",
    "        for tag in tag_data[i]:\n",
    "            new_line.append(tag[0])\n",
    "            new_line.append(tag[0])\n",
    "            new_line.append(tag[1])\n",
    "            new_line.append(tag[1])\n",
    "            new_line.append(tag[1])\n",
    "        return_data.append(np.array(new_line))\n",
    "    return np.array(return_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the combined data of desc and tags with no weighting to anything\n",
    "# COMBINED_WEIGHTED_DATA = CombineDescAndTagsWeighted(desc_data[0], tag_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the combined bag of words vector for combined weighted data\n",
    "# COMBINED_WEIGHTED_BAG_VECTOR = GetBagOfWordsVector(COMBINED_WEIGHTED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Below is the weighted bag data using the combined weighted data and weighted bag vector\n",
    "# WEIGHTED_BAG_DATA = ConvertDescToBagVector(COMBINED_WEIGHTED_DATA, COMBINED_WEIGHTED_BAG_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(desc_data[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
